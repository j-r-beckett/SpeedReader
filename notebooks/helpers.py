# /// script
# requires-python = ">=3.11"
# dependencies = ["marimo", "pandas"]
# ///
#
# WARNING: Do not edit this file in marimo. Edit as raw Python only.
# Marimo corrupts @app.function on save.

import marimo

__generated_with = "0.18.4"
app = marimo.App()

with app.setup:
    import marimo as mo
    import os
    import select
    import signal
    import subprocess
    import tempfile
    import time
    from datetime import datetime, timedelta, timezone
    from itertools import product
    from pathlib import Path

    import pandas as pd


def format_duration(seconds: float) -> str:
    """Format duration in human-readable form"""
    if seconds < 60:
        return f"{seconds:.0f}s"
    elif seconds < 3600:
        mins = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{mins}m {secs}s"
    else:
        hours = int(seconds // 3600)
        mins = int((seconds % 3600) // 60)
        return f"{hours}h {mins}m"


class PerfBandwidthMeasurement:
    """
    Measures memory bandwidth using perf stat.

    Usage:
        perf = start_perf_bandwidth()
        # ... run workload ...
        df = perf.stop()  # Returns DataFrame with timestamp, bandwidth_pct
    """

    def __init__(self):
        self._output_file = tempfile.mktemp(suffix=".csv")
        self._start_time = datetime.now(timezone.utc)
        self._proc = subprocess.Popen(
            [
                "perf", "stat", "-a",
                "-M", "tma_info_system_dram_bw_use",
                "-I", "250",
                "-x", ",",
                "-o", self._output_file,
            ],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
        )
        time.sleep(0.1)  # Let perf initialize

    def stop(self) -> pd.DataFrame:
        """Stop measurement and return DataFrame with timestamp + bandwidth_pct."""
        self._proc.send_signal(signal.SIGINT)
        self._proc.wait(timeout=5)
        time.sleep(0.2)  # Let perf flush output

        df = self._parse_output()
        os.unlink(self._output_file)
        return df

    def _parse_output(self) -> pd.DataFrame:
        rows = []
        with open(self._output_file) as f:
            for line in f:
                if "tma_info_system_dram_bw_use" in line:
                    parts = line.strip().split(",")
                    relative_secs = float(parts[0])
                    bandwidth_gbps = float(parts[6])
                    absolute_time = self._start_time + timedelta(seconds=relative_secs)
                    rows.append({
                        "timestamp": absolute_time,
                        "bandwidth_gbps": bandwidth_gbps,
                    })
        return pd.DataFrame(rows)


def start_perf_bandwidth() -> PerfBandwidthMeasurement:
    """Start measuring memory bandwidth. Call .stop() on the returned object to get results."""
    return PerfBandwidthMeasurement()


@app.function
def build_inference_benchmark() -> Path:
    """Build the MicroBenchmarks project and return the project path."""
    project_path = Path(__file__).parent.parent / "src" / "MicroBenchmarks"

    with mo.status.spinner(title="Building benchmark..."):
        build_result = subprocess.run(
            ["dotnet", "build", str(project_path), "-c", "Release", "--verbosity", "quiet"],
            capture_output=True,
            text=True,
        )
    if build_result.returncode != 0:
        raise RuntimeError(f"Build failed:\n{build_result.stdout}\n{build_result.stderr}")

    return project_path


def _run_inference_benchmark(
    project_path: Path,
    model: str,
    duration_seconds: float,
    batch_size: int,
    intra_threads: int,
    inter_threads: int,
    parallelism: int,
    warmup_seconds: float,
    on_tick,
) -> list[tuple[str, str]]:
    """
    Internal function to run inference benchmark.

    Returns list of (start_time, end_time) ISO timestamp pairs.
    on_tick() is called periodically (~10Hz) during execution for progress updates.
    """
    cmd = [
        "dotnet", "run",
        "--project", str(project_path),
        "--no-build", "-c", "Release",
        "--",
        "inference",
        "-m", model,
        "-d", str(duration_seconds),
        "-b", str(batch_size),
        "--intra-threads", str(intra_threads),
        "--inter-threads", str(inter_threads),
        "-p", str(parallelism),
        "-w", str(warmup_seconds),
    ]

    proc = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    )

    # Make stdout non-blocking
    fd = proc.stdout.fileno()
    os.set_blocking(fd, False)

    results = []
    buffer = ""

    while proc.poll() is None:
        # Wait for data with timeout for periodic progress updates
        ready, _, _ = select.select([proc.stdout], [], [], 0.1)
        on_tick()

        if ready:
            chunk = proc.stdout.read()
            if chunk:
                buffer += chunk
                while "\n" in buffer:
                    line, buffer = buffer.split("\n", 1)
                    line = line.strip()
                    if line:
                        start_ts, end_ts = line.split(",")
                        results.append((start_ts, end_ts))

    # Read any remaining output
    remaining = proc.stdout.read()
    if remaining:
        buffer += remaining
    for line in buffer.strip().split("\n"):
        line = line.strip()
        if line:
            start_ts, end_ts = line.split(",")
            results.append((start_ts, end_ts))

    if proc.returncode != 0:
        raise RuntimeError(f"Benchmark failed:\n{proc.stderr.read()}")

    return results


@app.function
def run_benchmark_sweep(
    project_path: Path,
    model: str | list[str],
    duration_seconds: float = 10.0,
    batch_size: int | list[int] = 1,
    intra_threads: int | list[int] = 1,
    inter_threads: int | list[int] = 1,
    parallelism: int | list[int] = 1,
    warmup_seconds: float = 2.0,
) -> pd.DataFrame:
    """
    Run inference benchmark over all combinations of parameters.

    Parameters that accept lists will be swept over (cartesian product).
    Returns a DataFrame with columns for each config parameter plus start_time and end_time.
    """
    def to_list(x):
        return x if isinstance(x, list) else [x]

    configs = [
        {"model": m, "batch_size": b, "intra_threads": intra, "inter_threads": inter, "parallelism": p}
        for m, b, intra, inter, p in product(
            to_list(model), to_list(batch_size), to_list(intra_threads),
            to_list(inter_threads), to_list(parallelism)
        )
    ]

    rows = []
    estimated_total = len(configs) * (warmup_seconds + duration_seconds + 1)
    start_time = time.time()

    with mo.status.spinner(title="Running benchmark...", remove_on_exit=True) as spinner:
        for cfg in configs:
            def on_tick():
                elapsed = time.time() - start_time
                remaining = max(0, estimated_total - elapsed)
                pct = min(99, int((elapsed / estimated_total) * 100))
                spinner.update(
                    title=f"Running benchmark... {pct}%",
                    subtitle=f"{format_duration(remaining)} remaining",
                )

            for start_str, end_str in _run_inference_benchmark(
                project_path=project_path,
                model=cfg["model"],
                duration_seconds=duration_seconds,
                batch_size=cfg["batch_size"],
                intra_threads=cfg["intra_threads"],
                inter_threads=cfg["inter_threads"],
                parallelism=cfg["parallelism"],
                warmup_seconds=warmup_seconds,
                on_tick=on_tick,
            ):
                rows.append({
                    **cfg,
                    "start_time": datetime.fromisoformat(start_str.replace("Z", "+00:00")),
                    "end_time": datetime.fromisoformat(end_str.replace("Z", "+00:00")),
                })

    return pd.DataFrame(rows)


if __name__ == "__main__":
    app.run()
