# /// script
# requires-python = ">=3.11"
# dependencies = ["marimo"]
# ///
#
# WARNING: Do not edit this file in marimo. Edit as raw Python only.
# Marimo corrupts @app.function on save.

import marimo

__generated_with = "0.18.4"
app = marimo.App()

with app.setup:
    import marimo as mo
    import subprocess
    from itertools import product
    from pathlib import Path


@app.function
def build_inference_benchmark() -> None:
    """Build the MicroBenchmarks project."""
    project_path = Path(__file__).parent.parent / "src" / "MicroBenchmarks"

    with mo.status.spinner(title="Building MicroBenchmarks..."):
        build_result = subprocess.run(
            ["dotnet", "build", str(project_path), "--verbosity", "quiet"],
            capture_output=True,
            text=True,
        )
    if build_result.returncode != 0:
        raise RuntimeError(f"Build failed:\n{build_result.stderr}")

    return True


@app.function
def run_inference_benchmark(
    model: str,
    batch_size: int = 1,
    intra_threads: int = 1,
    inter_threads: int = 1,
    parallelism: int = 1,
    iterations: int = 100,
    warmup: int = 10,
) -> list[float]:
    """
    Run inference benchmark and return timing data (milliseconds).

    Assumes project is already built (call build_inference_benchmark first).
    Shows progress bar during execution.
    """
    project_path = Path(__file__).parent.parent / "src" / "MicroBenchmarks"

    proc = subprocess.Popen(
        [
            "dotnet", "run",
            "--project", str(project_path),
            "--no-build",
            "--", "inference",
            "-m", model,
            "-b", str(batch_size),
            "--intra-threads", str(intra_threads),
            "--inter-threads", str(inter_threads),
            "-p", str(parallelism),
            "-n", str(iterations),
            "-w", str(warmup),
        ],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    )

    results = []
    with mo.status.progress_bar(total=iterations, title="Running benchmark") as bar:
        for line in proc.stdout:
            results.append(float(line.strip()))
            bar.update()

    proc.wait()
    if proc.returncode != 0:
        raise RuntimeError(f"Benchmark failed:\n{proc.stderr.read()}")

    return results


@app.function
def run_benchmark_sweep(
    model: str | list[str],
    batch_size: int | list[int] = 1,
    intra_threads: int | list[int] = 1,
    inter_threads: int | list[int] = 1,
    parallelism: int | list[int] = 1,
    iterations: int = 100,
    warmup: int = 10,
) -> list[tuple[dict, list[float]]]:
    """
    Run inference benchmark over all combinations of parameters.

    Parameters that accept lists will be swept over (cartesian product).
    Returns a list of (config_dict, measurements) tuples.

    The config dict contains the "what" (model, batch_size, threads, parallelism).
    iterations/warmup control "how" we measure and are not included in configs.
    """

    def to_list(x):
        return x if isinstance(x, list) else [x]

    models = to_list(model)
    batch_sizes = to_list(batch_size)
    intra_threads_list = to_list(intra_threads)
    inter_threads_list = to_list(inter_threads)
    parallelisms = to_list(parallelism)

    configs = [
        {
            "model": m,
            "batch_size": b,
            "intra_threads": intra,
            "inter_threads": inter,
            "parallelism": p,
        }
        for m, b, intra, inter, p in product(
            models, batch_sizes, intra_threads_list, inter_threads_list, parallelisms
        )
    ]

    results = []
    for cfg in configs:
        measurements = run_inference_benchmark(
            model=cfg["model"],
            batch_size=cfg["batch_size"],
            intra_threads=cfg["intra_threads"],
            inter_threads=cfg["inter_threads"],
            parallelism=cfg["parallelism"],
            iterations=iterations,
            warmup=warmup,
        )
        results.append((cfg, measurements))

    return results


if __name__ == "__main__":
    app.run()
