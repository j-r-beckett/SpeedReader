# /// script
# requires-python = ">=3.11"
# dependencies = ["marimo"]
# ///
#
# WARNING: Do not edit this file in marimo. Edit as raw Python only.
# Marimo corrupts @app.function on save.

import marimo

__generated_with = "0.18.4"
app = marimo.App()

with app.setup:
    import marimo as mo
    import os
    import select
    import subprocess
    import time
    from datetime import datetime
    from itertools import product
    from pathlib import Path

    import pandas as pd


def format_duration(seconds: float) -> str:
    """Format duration in human-readable form"""
    if seconds < 60:
        return f"{seconds:.0f}s"
    elif seconds < 3600:
        mins = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{mins}m {secs}s"
    else:
        hours = int(seconds // 3600)
        mins = int((seconds % 3600) // 60)
        return f"{hours}h {mins}m"


@app.function
def build_inference_benchmark() -> Path:
    """Build the MicroBenchmarks project and return the project path."""
    project_path = Path(__file__).parent.parent / "src" / "MicroBenchmarks"

    with mo.status.spinner(title="Building benchmark..."):
        build_result = subprocess.run(
            ["dotnet", "build", str(project_path), "-c", "Release", "--verbosity", "quiet"],
            capture_output=True,
            text=True,
        )
    if build_result.returncode != 0:
        raise RuntimeError(f"Build failed:\n{build_result.stdout}\n{build_result.stderr}")

    return project_path


def _run_inference_benchmark(
    project_path: Path,
    model: str,
    duration_seconds: float,
    batch_size: int,
    intra_threads: int,
    inter_threads: int,
    parallelism: int,
    warmup: float,
    on_tick,
) -> list[tuple[str, float]]:
    """
    Internal function to run inference benchmark.

    on_tick() is called periodically (~10Hz) during execution for progress updates.
    """
    cmd = [
        "dotnet", "run",
        "--project", str(project_path),
        "--no-build", "-c", "Release",
        "--",
        "inference",
        "-m", model,
        "-d", str(duration_seconds),
        "-b", str(batch_size),
        "--intra-threads", str(intra_threads),
        "--inter-threads", str(inter_threads),
        "-p", str(parallelism),
        "-w", str(warmup),
    ]

    proc = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    )

    # Make stdout non-blocking
    fd = proc.stdout.fileno()
    os.set_blocking(fd, False)

    results = []
    buffer = ""

    while proc.poll() is None:
        # Wait for data with timeout for periodic progress updates
        ready, _, _ = select.select([proc.stdout], [], [], 0.1)
        on_tick()

        if ready:
            chunk = proc.stdout.read()
            if chunk:
                buffer += chunk
                while "\n" in buffer:
                    line, buffer = buffer.split("\n", 1)
                    line = line.strip()
                    if line:
                        ts, duration = line.split(",")
                        results.append((ts, float(duration)))

    # Read any remaining output
    remaining = proc.stdout.read()
    if remaining:
        buffer += remaining
    for line in buffer.strip().split("\n"):
        line = line.strip()
        if line:
            ts, duration = line.split(",")
            results.append((ts, float(duration)))

    if proc.returncode != 0:
        raise RuntimeError(f"Benchmark failed:\n{proc.stderr.read()}")

    return results


@app.function
def run_benchmark_sweep(
    project_path: Path,
    model: str | list[str],
    duration_seconds: float = 10.0,
    batch_size: int | list[int] = 1,
    intra_threads: int | list[int] = 1,
    inter_threads: int | list[int] = 1,
    parallelism: int | list[int] = 1,
    warmup: float = 2.0,
) -> pd.DataFrame:
    """
    Run inference benchmark over all combinations of parameters.

    Parameters that accept lists will be swept over (cartesian product).
    Returns a DataFrame with columns for each config parameter plus timestamp and duration_ms.

    The config dict contains the "what" (model, batch_size, threads, parallelism).
    duration_seconds/warmup control "how" we measure and are not included in configs.
    """

    def to_list(x):
        return x if isinstance(x, list) else [x]

    models = to_list(model)
    batch_sizes = to_list(batch_size)
    intra_threads_list = to_list(intra_threads)
    inter_threads_list = to_list(inter_threads)
    parallelisms = to_list(parallelism)

    configs = [
        {
            "model": m,
            "batch_size": b,
            "intra_threads": intra,
            "inter_threads": inter,
            "parallelism": p,
        }
        for m, b, intra, inter, p in product(
            models, batch_sizes, intra_threads_list, inter_threads_list, parallelisms
        )
    ]

    rows = []
    config_duration = warmup + duration_seconds + 1  # +1s overhead per config
    total_duration = len(configs) * config_duration

    start_time = time.time()
    with mo.status.spinner(title="Running benchmark... 0%", remove_on_exit=True) as spinner:
        mo.output.append(mo.md(f"Estimated: {format_duration(total_duration)}"))
        for cfg in configs:

            def on_tick():
                elapsed = time.time() - start_time
                pct = min(99, int((elapsed / total_duration) * 100))
                spinner.update(title=f"Running benchmark... {pct}%")

            measurements = _run_inference_benchmark(
                project_path=project_path,
                model=cfg["model"],
                duration_seconds=duration_seconds,
                batch_size=cfg["batch_size"],
                intra_threads=cfg["intra_threads"],
                inter_threads=cfg["inter_threads"],
                parallelism=cfg["parallelism"],
                warmup=warmup,
                on_tick=on_tick,
            )

            for ts_str, duration_ms in measurements:
                ts = datetime.fromisoformat(ts_str.replace("Z", "+00:00"))
                rows.append({
                    **cfg,
                    "timestamp": ts,
                    "duration_ms": duration_ms,
                })

    actual_time = time.time() - start_time
    mo.output.append(mo.md(f"Actual: {format_duration(actual_time)}"))

    return pd.DataFrame(rows)


if __name__ == "__main__":
    app.run()
